\subsection{BERT Model}
BERT is a transformer-based language model that has pushed 
state-of-the-art results in many NLP tasks. The BERT architecture
comprises $L$ encoder layers with $H$ attention heads in each layer.
The input of each attention head is a matrix $X$ consisting of the
$d$-dimensional representations (row-wise) of $m$ tokens, so that 
$X$ is of shape $m\times d$. The head outputs an updated 
representation matrix $X^{\textrm{out}}$: 
\begin{equation}
    X^\mathrm{out} = W^\mathrm{attn}(XW^V), \textrm{ with } 
    W^\mathrm{attn} = \mathrm{softmax}\bigl( \frac{(XW^Q)(XW^K)^T}{\sqrt{d}} \bigr)
\end{equation}
where $W^Q, W^K, W^V$ are trained projection matrices of shape $d\times d$
and $W^\mathrm{attn}$ is of shape $m\times m$ matrix of attention weights.
Each element $w_{ij}^\mathrm{attn}$ can be interpreted as a weight of the
$j$-th input's {\it relation} to the $i$-th output: larger weights mean 
stronger connection between the two tokens.

\subsection{Attention Graph}
An {\it attention graph} is a weighted graph representation of an attention matrix $W^{attn}$, 
in which the vertices represent the tokens and the edges connect a pair of tokens if the 
corresponding weight is higher than the predefined threshold value. 

This threshold value is set to distinguish weak and strong relations between tokens. 
But the choice of the threshold value seems to be a very hard problem. 
Moreover, varying the threshold, the graph structure may change dramatically. 

Hopefully, TDA methods can extract the properties of graph structure, without specifying the
concrete value of the threshold.

\subsection{Topological Data Analysis}
Topological data analysis is a young and rapidly evolving field that applies some of the 
powerful methods from algebraic topology to data science. There are already exist a plethora of
good tutorials and surveys for non-mathematicians \cite{murugan2019introductiontopologicaldataanalysis,tda4ds}
as well as for those who has a mathematical background \cite{Edelsbrunner2008,Oudot2015,Schenck2022}.

The main instrument in topological data analysis, {\it persistent homology}, 
allows one to track the changes in the topological structure for different objects, such as
point clouds, scalar functions, images, weighted graphs \cite{adams2016persistenceimagesstablevector,Aktas2019}. 

Specifically, in our work, we are given with a set of tokens $V$ and an attention matrix $W$.
Next, we build a family of attention graphs, which is being indexed by increasing threshold values
This family is called a {\it filtration}, and it is a crucial object in TDA. 

When such seqeunce of graphs is given, the persistent homology in dimension $0$ and $1$ are computed.
Dimension $0$ illustrates if there are connected components or clusters in our data, and 
dimension $1$ shows if our data possesses cycles or <<loops>>. 
These calculations provide us with the {\it persistence diagram}, which can be further used to
derive specific topological features, such as the amount of connected components or the amount of
cycles (such features are also called the {\it Betti numbers}).
