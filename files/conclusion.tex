This paper introduces a novel approach for the vulnerability detection task based on TDA.
We propose the set of interpretable topological features, obtained from the persistence diagrams,
that are derived from the attention matrices of any transformer-based LM. The experiments show that
the ML classifiers, trained on these features, perform on par with pre-trained on code LLM.

We are publicly releasing our code, hoping to stimulate the research on the TDA-based methods to
vulnerability detection and other NLP tasks. A potential direction for future work is to combine
topological features that encode semantics with the topological features that encode structural information.
It is also interesting to investigate the topology of different symmetrisations of the attention matrices
and how they encode the semantics. Another interesting direction is to incorporate 
multiparameter persistent homology to the study of the semantic evolution in the attention heads.

\subsubsection*{Acknowledgements}
The work was carried out as part of the state assignment N 1021061609772-0-1.2.1
(FFNU-2024-0020).
