\subsubsection*{Methodology} In order to test whether the topological information encode can
be used for vulnerability detection, we train both SVM and LightGBM classifier over topological features 
derived from the attention matrices from the BERT model as described in Section \ref*{features}. 
The training details are outlined in Appendix.

\subsubsection*{Data} We train and evaluate our classifier on {\it Devign} dataset.
This dataset is collected from 2 large C-language open-source projects that are popular 
among developers and diversified in functionality, i.e., QEMU, and FFmpeg. 
Due to our computational resources, we were only using those data samples, that, 
being tokenized, are of length less than 150. In this case, the point cloud that is being 
constructed during the attention symmetrization, is bounded to be length less than 150.

\subsubsection*{Baselines} We use \texttt{microsoft/codebert-base} \cite{feng2020codebert} model from the HuggingFace library \cite{huggingface}
as the pre-trained on code dataset BERT-based baseline. We also fully fine-tune \texttt{microsoft/codebert-base}.
