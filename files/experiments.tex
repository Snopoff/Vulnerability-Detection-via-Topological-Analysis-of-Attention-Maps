\subsubsection*{Methodology} 
To evaluate whether the encoded topological information can be used for 
vulnerability detection, we train Logistic Regression, Support Vector Machine (SVM), 
and Gradient Boosting classifiers on the topological features derived from the 
attention matrices of the BERT model, as described in Section \ref*{features}. 
We utilize the {\it scikit-learn} library \cite{scikit-learn} for Logistic Regression 
and SVM, and the {\it LightGBM} library \cite{ke2017lightgbm} for Gradient Boosting. 
Detailed training procedures are outlined in Appendix \ref*{training_details}.

\subsubsection*{Data} 
We train and evaluate our classifier on {\it Devign} dataset.
This dataset comprises samples from two large, widely-used open-source C-language projects: QEMU, and FFmpeg, 
which are popular among developers and diverse in functionality.
Due to computational constraints, we were only using those data samples, that, 
being tokenized, are of length less than $150$. This ensures that the point cloud 
constructed during attention symmetrization is also limited to a maximum length of $150$.

\subsubsection*{Baselines} 
We employ the \texttt{microsoft/codebert-base} model \cite{feng2020codebert} from the 
HuggingFace library \cite{huggingface} as our pre-trained BERT-based baseline. 
Additionally, we fully fine-tune the \texttt{microsoft/codebert-base} model for comparison.