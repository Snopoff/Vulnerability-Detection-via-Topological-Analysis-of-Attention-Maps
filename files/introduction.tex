The problem of the source code vulnerability detection becomes an important problem in the field
of software engineering nowadays. This is a complex task involving the analysis of both syntax
and semantics of the source code. The static analysis methods were dominating during most of the time
in the vulnerability detection. The static tools tend to rely heavily only on the syntax of the program,
thus loosing the potential information that is hidden in the semantics. Therefore such approaches
suffer from high rates of false positives.

But recently the different ML-based and DL-based methods started to
appear \cite{sharma_survey_2024,steenhoek_empirical_2023}. 
Most DL-based solutions provide the methods based on graph neural networks (GNNs) and large language models (LLMs).
Both GNN-based and LLM-based methods are targeted to learn the semantics together with the syntax 
of the program. This approaches show the promising results, beating the traditional static analysis tools and 
demonstating low rates of false positives as well as false negatives \cite{katsadouros_survey_2022}.

Our work demonstates a novel approach to the vulnerability detection problem. As mentioned above,
the superiority of DL-based models over static ones are potentially due to the use of the semantic
information of the program. This semantic information is captured somehow with neural networks 
during training or fine-tuning. Moreover, LLM-based models should capture this information even
wihout any fine-tune.

On the other hand, as well as with the syntax of the source code, the semantics can also be represented
as a tree or a graph. Such representation opens the doors for the use of topological methods.

In this work, we leverage the BERT model, pretrained on the source code, for capturing 
the semantic information of the programs, and we also use the tools from topological data 
analysis \cite{tda4ds} to retrieve the relations in this semantics. 
This is an attempt to analyse the interpret the attention matrices via topological instruments.

Our work is inspired by the works of Laida Kushareva et. al. \cite{kushnareva_artificial_2021}, 
in which the authors apply the topological methods for the artificial text detection, 
and show that the topology of the semantics captured by a LLM model (such as BERT) provides enough 
information for the successful classification, beating neural baselines and performing on par with 
a fully fine-tuned BERT model, but being more robust towards the unseen data.
