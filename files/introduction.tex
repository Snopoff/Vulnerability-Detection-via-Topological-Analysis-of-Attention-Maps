The problem of the source code vulnerability detection becomes an important problem in the field
of software engineering nowadays. This is a complex task involving the analysis of both syntax
and semantics of the source code. The static analysis methods were dominating during most of the time
in the vulnerability detection. The static tools tend to rely heavily only on the syntax of the program,
thus loosing the potential information that is hidden in the semantics. Therefore such approaches
suffer from high rates of false positives.

But recently the different ML-based and DL-based methods started to
appear \cite{sharma_survey_2024, steenhoek_empirical_2023}. 
Most DL-based solutions provide the methods based on graph neural networks (GNNs) \cite{wen_vulnerability_2023, nguyen_regvd_2022, zhou_devign_2019}.
Besides GNN-based models, there exists LLM-based approaches \cite{shestov_finetuning_2024, steenhoek_comprehensive_2024, guo_graphcodebert_2021}. 
Both GNN-based and LLM-based methods are targeted to learn the semantics together with the syntax 
of the program. This approaches show the promising results, in particular such methods demonstrate 
low rates of false positives as well as false negatives \cite{katsadouros_survey_2022}.

However some studies show that the generalization of DL-based methods is very poor, and that 
there are no existing models that would perform well in real-world settings \cite{saikat_chakraborty_deep_2022}. 
This situation shows that there is still no silver bullet for the vulnerability detection task. 

Our work demonstates a novel approach to the vulnerability detection problem. As mentioned above,
the superiority of DL-based models over static ones are potentially due to the use of the semantic
information of the program. This semantic information is captured somehow with neural networks 
during training or fine-tuning. Moreover, LLM-based models should capture this information even
wihout any fine-tune.

On the hand, as well as with the syntax of the source code, the semantics can also be represented
as a tree or a graph. Such representation opens the doors for the use of topological methods.

In this work, we leverage the BERT model, pretrained on the source code, for capturing 
the semantic information of the programs, and we also use the tools from topological data 
analysis \cite{something-1} to retrieve the relations in this semantics. 
This is an attempt to analyse the interpret the attention matrices via topological instruments.

Our work is 
inspired by the works of Laida Kushareva et. al. \cite{kushnareva_artificial_2021}, in which the 
authors apply the topological methods for the artificial text detection, and show that the topology
of the semantics captured by a LLM model (such as BERT) provides enough information for the 
successful classification, beating neural baselines and performing on par with a fully fine-tuned
BERT model, but being more robust towards the unseen data.
