\subsection{BERT Model}
BERT (Bidirectional Encoder Representations from Transformers) is a 
transformer-based language model that has set new benchmarks in various natural 
language processing (NLP) tasks. The BERT architecture
consists of $L$ encoder layers, each with $H$ attention heads.
Each attention head receives a matrix $X$ as input, representing the 
$d$-dimensional embeddings of $m$ tokens, so that 
$X$ is of shape $m\times d$. The head outputs an updated 
representation matrix $X^{\textrm{out}}$: 
\begin{equation}
    X^\mathrm{out} = W^\mathrm{attn}(XW^V), \textrm{ with } 
    W^\mathrm{attn} = \mathrm{softmax}\bigl( \frac{(XW^Q)(XW^K)^T}{\sqrt{d}} \bigr).
\end{equation}
Here $W^Q, W^K, W^V$ are trained projection matrices of shape $d\times d$
and $W^\mathrm{attn}$ is matrix of attention weights with shape $m\times m$ 
Each element $w_{ij}^\mathrm{attn}$ can be interpreted as a weight of the
$j$-th input's {\it relation} to the $i$-th output where larger weights 
indicate a stronger connection between the two tokens.

\subsection{Attention Graph}
An {\it attention graph} is a weighted graph representation of an attention matrix $W^{attn}$, 
where vertices represent the tokens and the edges connect a pair of tokens if the 
corresponding weight exceeds a predefined threshold value.

Setting this threshold value is critical yet challenging, as it distinguishes weak 
and strong relations between tokens. Additionally, varying the threshold can 
significantly alter the graph structure.

Topological data analysis (TDA) methods can extract properties of the graph structure 
without specifying an exact threshold value, addressing this challenge effectively.

\subsection{Topological Data Analysis}
Topological data analysis (TDA) is an emerging field that applies 
algebraic topology methods to data science. There are numerous excellent tutorials 
and surveys available for both 
non-mathematicians \cite{murugan2019introductiontopologicaldataanalysis,tda4ds} and 
those with a mathematical background \cite{Edelsbrunner2008,Oudot2015,Schenck2022}.

The main tool in topological data analysis, {\it persistent homology}, 
tracks changes in the topological structure of various objects, such as point clouds, 
scalar functions, images, and weighted graphs \cite{adams2016persistenceimagesstablevector,Aktas2019}.

In our work, given a set of tokens $V$ and an attention matrix $W$,we construct a 
family of attention graphs indexed by increasing threshold values. This family, known 
as a {\it filtration}, is a fundamental object in TDA

With this sequence of graphs, we compute persistent homology in dimensions $0$ and $1$.
Dimension $0$ reveals connected components or clusters in the data, while 
dimension $1$ identifies cycles or <<loops>>. 
These computations yield a {\it persistence diagram}, which can be used to derive 
specific topological features, such as the number of connected components and 
cycles (such features are also called the {\it Betti numbers}) 
(see Appendix \ref*{persistent_homology} for the details).
