This paper introduces a novel approach for the vulnerability detection task based on 
topological data analysis (TDA). We propose a set of interpretable topological 
features, obtained from persistence diagrams derived from the attention matrices of 
any transformer-based language model. The experiments demonstrate that machine 
learning classifiers trained on these features perform comparably to pre-trained 
code-specific large language models (LLMs).

We are making our code publicly available\footnote{https://github.com/Snopoff/Vulnerability-Detection-via-Topological-Analysis-of-Attention-Maps} with the aim of 
encouraging research into TDA-based methods for vulnerability detection and other NLP tasks. 
A potential direction for future work is to combine topological features that 
encode semantics with topological features that encode structural information. 
It is also interesting to investigate the topology of different symmetrizations of 
the attention matrices and how they encode semantics. Another promising direction is 
to incorporate multiparameter persistent homology to study the semantic evolution in 
the attention heads.

\subsubsection*{Acknowledgements}
The work was carried out as part of the state assignment N 1021061609772-0-1.2.1
(FFNU-2024-0020).