This paper introduces a novel approach for the vulnerability detection task based on 
topological data analysis (TDA). We propose a set of interpretable topological 
features, obtained from persistence diagrams derived from the attention matrices of 
any transformer-based language model. The experiments demonstrate that machine 
learning classifiers trained on these features outperform pre-trained 
code-specific large language models (LLMs).

Our code is publicly available\footnote{https://github.com/Snopoff/Vulnerability-Detection-via-Topological-Analysis-of-Attention-Maps}, and we encourage further research into TDA-based methods for vulnerability detection and other NLP tasks. Future work could explore combining topological features that encode semantics with those capturing structural information. Additionally, studying different symmetrizations of attention matrices and how they encode semantics could provide new insights. Another interesting direction is applying multiparameter persistent homology to analyze the semantic evolution in attention heads.
\subsubsection*{Acknowledgements}
The work was carried out as part of the state assignment N 1021061609772-0-1.2.1
(FFNU-2024-0020).