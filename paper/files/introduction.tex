The problem of source code vulnerability detection has become increasingly 
important in the field of software engineering. This complex task involves analyzing 
both the syntax and semantics of the source code. Traditionally, static analysis 
methods have dominated the field of vulnerability detection. 
These static tools tend to rely heavily on the syntax of the program, thereby 
overlooking potential information hidden in the semantics. As a result, 
such approaches often suffer from high rates of false positives.

Recently, however, various machine learning (ML) and deep learning (DL) methods have 
emerged \cite{sharma_survey_2024,steenhoek_empirical_2023}. Most DL-based solutions 
leverage graph neural networks (GNNs) and large language models (LLMs). 
These methods aim to learn both the semantics and syntax of the program. 
They have shown promising results, outperforming traditional static analysis tools and 
demonstrating low rates of false positives and false 
negatives \cite{katsadouros_survey_2022}.

Our work presents a novel approach to the vulnerability detection problem.
As previously mentioned, the superiority of DL-based models over static ones is 
potentially due to their ability to capture the semantic information of the program. 
This semantic information is captured by the neural networks during training or 
fine-tuning. Furthermore, LLM-based models can capture this information even without 
fine-tuning.

On the other hand, the semantics of the source code, like its syntax, can also be represented as a 
tree or a graph. This representation allows for the application of topological methods.

In this work, we leverage the BERT model, pre-trained on source code, to capture the 
semantic information of programs. We also utilize tools from topological 
data analysis (TDA) \cite{tda4ds} to explore the relationships within this semantic 
information. Our aim is to analyze and interpret the attention matrices using 
topological instruments.

Our work is inspired by the work of Laida Kushareva et. al. \cite{kushnareva_artificial_2021}, 
in which the authors apply the topological methods for the artificial text detection. They demonstrated that 
the topology of the semantics captured by an LLM model (such as BERT) provides 
sufficient information for successful classification. Their approach outperformed 
neural baselines and performed on par with a fully fine-tuned BERT model, while being 
more robust to unseen data.

